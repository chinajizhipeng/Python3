神经网络：梯度下降（优化方法）
input layer
hidden layer1
hidden layer2
output layer

import tensorflow as tf
import numpy as np
####################################e.g.1
#creat dara USE FLOAT32
x_data = np.random.rand(100).astype(np.float32)
y_data = x_data*0.1+0.3
#create tensorflow structure start
Weights = tf.Variable(tf.random_uniform([1],-1.0,1.0)) #定义参数变量 1维 随机变量生成（-1，1）
biases = tf.Variable(tf.zeros([1]))

y = Weights*x_data+biases #预测的y

loss = tf.reduce_mean(tf.square(y-y_data)) #真实值和预测值之间的差别
optimizer = tf.train.GradientDescentOptimizer(0.5) #创建优化器，学习效率0.5
train = optimizer.minimize(loss) #用优化器减少误差

# init = tf.initialize_all_variables() # 初始化结构，tf 马上就要废弃这种写法
init = tf.global_variables_initializer()  # 替换成这样就好
#create tensorflow structure end
sess = tf.Session() #用 Session 来执行 init 初始化步骤. 并且, 用 Session 来 run 每一次 training 的数据. 逐步提升神经网络的预测准确性.
sess.run(init)          # Very important

for step in range(201):
    sess.run(train)
    if step % 20 == 0:
        print(step, sess.run(Weights), sess.run(biases)) #打印参数
##Session
#Session 是 Tensorflow 为了控制,和输出文件的执行的语句. 运行 session.run() 可以获得你要得知的运算结果, 或者是你所要运算的部分.
import tensorflow as tf
matrix1 = tf.constant([[3,3]])   #两行一列
matrix2 = tf.constant([[2],[3]]) #一行两列
product = tf.matmul(matrix1,matrix2) #矩阵乘法 np.dot(mi,m2)
#method 1
sess = tf.Session()
result = sess.run(product)
print(result)
sess.close()
#method 2
with tf.Session() as sess:
    result2 = sess.run(product)
    print(result2)
##Variable
只有定义变量，才是变量
import tensorflow as tf
state = tf.Variable(0, name='counter')
one = tf.constant(1) # 定义常量 one
new_value = tf.add(state, one) # 定义加法步骤 (注: 此步并没有直接计算)
update = tf.assign(state, new_value) # 将 State 更新成 new_value
init = tf.global_variables_initializer()  # 如果定义 Variable, 就一定要 initialize
with tf.Session() as sess: # 使用 Session激活init
    sess.run(init)
    for _ in range(3):
        sess.run(update)
        print(sess.run(state)) 直接 print(state) 不起作用！！ 一定要把 sess 的指针指向 state 再进行 print 才能得到想要的结果！
##placeholder  placeholder 是 Tensorflow 中的占位符，暂时储存变量.Tensorflow 如果想要从外部传入data, 那就需要用到 tf.placeholder(), 然后以这种形式传输数据 sess.run(***, feed_dict={input: **}).
#在 Tensorflow 中需要定义 placeholder 的 type ，一般为 float32 形式
input1 = tf.placeholder(tf.float32)
input2 = tf.placeholder(tf.float32)
ouput = tf.multiply(input1, input2) # mul = multiply 是将input1和input2 做乘法运算，并输出为 output 
with tf.Session() as sess:
    print(sess.run(ouput, feed_dict={input1: [7.], input2: [2.]})) 需要传入的值放在了feed_dict={} 并一一对应每一个 input. placeholder 与 feed_dict={} 是绑定在一起出现的。
##添加层 def add_layer()
import tensorflow as tf
def add_layer(inputs, in_size, out_size, activation_function=None):  #输入值、输入的大小、输出的大小和激励函数，我们设定默认的激励函数是None   
    Weights = tf.Variable(tf.random_normal([in_size, out_size])) #weights为一个in_size行, out_size列的随机变量矩阵。
    biases = tf.Variable(tf.zeros([1, out_size]) + 0.1)
    Wx_plus_b = tf.matmul(inputs, Weights) + biases #tf.matmul()是矩阵的乘法。
    if activation_function is None: 激励函数为None时，输出就是当前的预测值
        outputs = Wx_plus_b
    else: #不为None时，就把Wx_plus_b传到activation_function()函数中得到输出。
        outputs = activation_function(Wx_plus_b) 
    return outputs #返回输出

#####################################eg.2
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
def add_layer(inputs, in_size, out_size, activation_function=None):  #输入值、输入的大小、输出的大小和激励函数，我们设定默认的激励函数是None
    Weights = tf.Variable(tf.random_normal([in_size, out_size])) #weights为一个in_size行, out_size列的随机变量矩阵。
    biases = tf.Variable(tf.zeros([1, out_size]) + 0.1)
    Wx_plus_b = tf.matmul(inputs, Weights) + biases #tf.matmul()是矩阵的乘法。
    if activation_function is None: #激励函数为None时，输出就是当前的预测值
        outputs = Wx_plus_b
    else: #不为None时，就把Wx_plus_b传到activation_function()函数中得到输出。
        outputs = activation_function(Wx_plus_b)
    return outputs #返回输出
#神经元=变量？
x_data = np.linspace(-1,1,300, dtype=np.float32)[:, np.newaxis]
noise = np.random.normal(0, 0.05, x_data.shape).astype(np.float32)
y_data = np.square(x_data) - 0.5 + noise

xs = tf.placeholder(tf.float32, [None, 1])
ys = tf.placeholder(tf.float32, [None, 1])
##搭建网络，输入层和输出层的神经元是规定好的，输入输出几个变量就是各几层
#输入层
l1 = add_layer(xs, 1, 10, activation_function=tf.nn.relu) #1层，就是xs的神经元数量，10层隐藏层神经元
#输出层
prediction = add_layer(l1,10,1,activation_function=None) #第一个是隐藏层的size，第二个是y_data的size

loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction),
                     reduction_indices=[1])) #计算预测值prediction和真实值的误差，对二者差的平方求和再取平均。
#对误差进行提升
train_step = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(loss) #通常小于1
#对所有变量进行初始
init = tf.initialize_all_variables()

sess = tf.Session()
sess.run(init)

for i in range(1000):
    sess.run(train_step,feed_dict={xs:x_data,ys:y_data})
    if i % 50 == 0:
        print(sess.run(loss,feed_dict={xs:x_data,ys:y_data}))

##################分类








