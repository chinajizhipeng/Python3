神经网络：梯度下降（优化方法）
#误差方程：计算预测出的值与实际的值有多大差别 平方差
#梯度下降 找出梯度最低的点，梯度线躺平的地方（误差最小的点）
input layer
hidden layer1
hidden layer2
output layer

import tensorflow as tf
import numpy as np
####################################e.g.1
#creat dara USE FLOAT32
x_data = np.random.rand(100).astype(np.float32)
y_data = x_data*0.1+0.3
#create tensorflow structure start
Weights = tf.Variable(tf.random_uniform([1],-1.0,1.0)) #定义参数变量 1维 随机变量生成（-1，1）
biases = tf.Variable(tf.zeros([1])) #假定截距
y = Weights*x_data+biases #预测的y
loss = tf.reduce_mean(tf.square(y-y_data)) #真实值和预测值之间的差别
optimizer = tf.train.GradientDescentOptimizer(0.5) #创建优化器，学习效率0.5
train = optimizer.minimize(loss) #用优化器减少误差

# init = tf.initialize_all_variables() # 初始化结构，tf 马上就要废弃这种写法
init = tf.global_variables_initializer()  # 替换成这样就好
#create tensorflow structure end
sess = tf.Session() #用 Session 来执行 init 初始化步骤. 并且, 用 Session 来 run 每一次 training 的数据. 逐步提升神经网络的预测准确性.
sess.run(init)          # Very important

for step in range(201):
    sess.run(train)
    if step % 20 == 0:
        print(step, sess.run(Weights), sess.run(biases)) #打印参数
### 多维 2个变量
#creat dara USE FLOAT32
x1_data = np.random.rand(100).astype(np.float32)
x2_data = np.random.rand(100).astype(np.float32)
x_data = np.column_stack((x1_data,x2_data)).T  #将列向量变为行向量 专职
# x_data = np.float32(np.random.rand(2, 100))
y_data = np.dot([0.200, 0.300], x_data) + 0.300
#create tensorflow structure start
Weights = tf.Variable(tf.random_uniform([1,2], -1.0, 1.0)) #定义参数变量2个 随机变量生成（-1，1）
biases = tf.Variable(tf.zeros([1])) #假定截距
y = tf.matmul(Weights, x_data) + biases #预测的y
loss = tf.reduce_mean(tf.square(y-y_data)) #真实值和预测值之间的差别
optimizer = tf.train.GradientDescentOptimizer(0.5) #创建优化器，学习效率0.5
train = optimizer.minimize(loss) #用优化器减少误差

# init = tf.initialize_all_variables() # 初始化结构，tf 马上就要废弃这种写法
init = tf.global_variables_initializer()  # 替换成这样就好
#create tensorflow structure end
sess = tf.Session() #用 Session 来执行 init 初始化步骤. 并且, 用 Session 来 run 每一次 training 的数据. 逐步提升神经网络的预测准确性.
sess.run(init)          # Very important

for step in range(2010):
    sess.run(train)
    if step % 200 == 0:
        print(step, sess.run(Weights), sess.run(biases)) #打印参数
###Session
#Session 是 Tensorflow 为了控制,和输出文件的执行的语句. 运行 session.run() 可以获得你要得知的运算结果, 或者是你所要运算的部分.
import tensorflow as tf
matrix1 = tf.constant([[3,3]])   #两行一列
matrix2 = tf.constant([[2],[3]]) #一行两列
product = tf.matmul(matrix1,matrix2) #矩阵乘法 np.dot(mi,m2)
#method 1
sess = tf.Session()
result = sess.run(product)
print(result)
sess.close()
#method 2
with tf.Session() as sess:
    result2 = sess.run(product)
    print(result2)
##Variable
#只有定义变量，才是变量
import tensorflow as tf
state = tf.Variable(0, name='counter')
state.name #变量的名字
one = tf.constant(1) # 定义常量 one 变量加常量还等于常量
new_value = tf.add(state, one) # 定义加法步骤 (注: 此步并没有直接计算)
update = tf.assign(state, new_value) # 将 State 更新成 new_value
init = tf.global_variables_initializer()  # 如果定义 Variable, 就一定要 initialize
with tf.Session() as sess: # 使用 Session激活init
    sess.run(init)
    for _ in range(3):
        sess.run(update)
        print(sess.run(state)) #直接 print(state) 不起作用！！ 一定要把 sess 的指针指向 state 再进行 print 才能得到想要的结果！
##placeholder  placeholder 是 Tensorflow 中的占位符，暂时储存变量.Tensorflow 如果想要从外部传入data, 那就需要用到 tf.placeholder(), 然后以这种形式传输数据 sess.run(***, feed_dict={input: **}).
#在 Tensorflow 中需要定义 placeholder 的 type ，一般为 float32 形式
input1 = tf.placeholder(tf.float32) #先定义，后传值
input2 = tf.placeholder(tf.float32)
ouput = tf.multiply(input1, input2) # mul = multiply 是将input1和input2 做乘法运算，并输出为 output 
with tf.Session() as sess:
    print(sess.run(ouput, feed_dict={input1: [7.], input2: [2.]})) #需要传入的值放在了feed_dict={} 并一一对应每一个 input. placeholder 与 feed_dict={} 是绑定在一起出现的。
#激励函数 把线性函数掰弯 relu sigmod tanh 卷积用relu 循环 relu or tanh 
tf.nn.relu(feature)
tf.sigmod(x)
tf.tanh(x)
##添加层 def add_layer()
import tensorflow as tf
def add_layer(inputs, in_size, out_size, activation_function=None):  #输入值、输入的大小、输出的大小和激励函数，我们设定默认的激励函数是None   
    Weights = tf.Variable(tf.random_normal([in_size, out_size])) #weights为一个in_size行, out_size列的随机变量矩阵。
    biases = tf.Variable(tf.zeros([1, out_size]) + 0.1) #biases的推荐值不为0
    Wx_plus_b = tf.matmul(inputs, Weights) + biases #神经网络未激活的值
    if activation_function is None: #激励函数为None时，输出就是当前的预测值
        outputs = Wx_plus_b
    else: #不为None时，就把Wx_plus_b传到activation_function()函数中得到输出。
        outputs = activation_function(Wx_plus_b) 
    return outputs #返回输出

#####################################eg.2
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
def add_layer(inputs, in_size, out_size, activation_function=None): #输入值、输入的大小、输出的大小和激励函数，我们设定默认的激励函数是None
    Weights = tf.Variable(tf.random_normal([in_size, out_size])) 	#weights为一个in_size行, out_size列的随机变量矩阵。
    biases = tf.Variable(tf.zeros([1, out_size]) + 0.1)
    Wx_plus_b = tf.matmul(inputs, Weights) + biases 				#tf.matmul()是矩阵的乘法。
    if activation_function is None: 								#激励函数为None时，输出就是当前的预测值
        outputs = Wx_plus_b
    else: #不为None时，就把Wx_plus_b传到activation_function()函数中得到输出。
        outputs = activation_function(Wx_plus_b)
    return outputs 		#返回输出
#输入层的神经元个数=变量的个数  输出层的神经元个数等于被解释变量的个数
x_data = np.linspace(-1,1,300, dtype=np.float32)[:, np.newaxis] 	#300行1列
noise = np.random.normal(0, 0.05, x_data.shape).astype(np.float32)
y_data = np.square(x_data) - 0.5 + noise

xs = tf.placeholder(tf.float32, [None, 1])
ys = tf.placeholder(tf.float32, [None, 1])
##搭建网络，输入层和输出层的神经元是规定好的，输入输出几个变量就是各几个神经元
#输入层
l1 = add_layer(xs, 1, 10, activation_function=tf.nn.relu) 	#1层，就是xs的神经元数量，10层隐藏层神经元
#输出层
prediction = add_layer(l1,10,1,activation_function=None) 	#第一个是隐藏层的size，第二个是y_data的输出变量的个数

loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction),
                     reduction_indices=[1])) #计算预测值prediction和真实值的误差，对二者差的平方求和再取平均。
#对误差进行提升
train_step = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(loss) #通常小于1 
#对所有变量进行初始
init = tf.initialize_all_variables()

sess = tf.Session()
sess.run(init)

for i in range(1000):
    sess.run(train_step,feed_dict={xs:x_data,ys:y_data})
    if i % 50 == 0:
        print(sess.run(loss,feed_dict={xs:x_data,ys:y_data}))
##结果可视化 
##优化器

##################分类
import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data #下载分类数据 1-10数字识别
# number 1 to 10 data
mnist = input_data.read_data_sets('MNIST_data', one_hot=True)
def add_layer(inputs, in_size, out_size, activation_function=None,):
    # add one more layer and return the output of this layer
    Weights = tf.Variable(tf.random_normal([in_size, out_size]))
    biases = tf.Variable(tf.zeros([1, out_size]) + 0.1,)
    Wx_plus_b = tf.matmul(inputs, Weights) + biases
    if activation_function is None:
        outputs = Wx_plus_b
    else:
        outputs = activation_function(Wx_plus_b,)
    return outputs

def compute_accuracy(v_xs, v_ys): #添加精度 
    global prediction
    y_pre = sess.run(prediction, feed_dict={xs: v_xs})
    correct_prediction = tf.equal(tf.argmax(y_pre,1), tf.argmax(v_ys,1)) #10个概率取最大，看是否相等
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
    result = sess.run(accuracy, feed_dict={xs: v_xs, ys: v_ys})
    return result

# define placeholder for inputs to network
xs = tf.placeholder(tf.float32, [None, 784]) # 28x28
ys = tf.placeholder(tf.float32, [None, 10])  #输出10个数字

# add output layer 只有输入层和输出层。
prediction = add_layer(xs, 784, 10,  activation_function=tf.nn.softmax) 

# the error between prediction and real data
#。交叉熵用来衡量预测值和真实值的相似程度，如果完全相同，它们的交叉熵等于零。
cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction),
                                              reduction_indices=[1]))       # loss
#train方法（最优化算法）采用梯度下降法。
train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)

sess = tf.Session()
# important step
# tf.initialize_all_variables() no long valid from
# 2017-03-02 if using tensorflow >= 0.12
if int((tf.__version__).split('.')[1]) < 12 and int((tf.__version__).split('.')[0]) < 1:
    init = tf.initialize_all_variables()
else:
    init = tf.global_variables_initializer()
sess.run(init)

for i in range(1000):
    batch_xs, batch_ys = mnist.train.next_batch(100) #每次取100个数据
    sess.run(train_step, feed_dict={xs: batch_xs, ys: batch_ys})
    if i % 50 == 0:
        print(compute_accuracy(
            mnist.test.images, mnist.test.labels))







